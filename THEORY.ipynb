{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV9PMVR3ydbK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN THEORY\n",
        "Convolutional Neural Network (CNN):\n",
        "\n",
        "CNN is a type of deep learning neural network primarily used for image recognition and computer vision tasks.\n",
        "It applies convolutional layers to automatically learn spatial hierarchies of patterns within an image.\n",
        "\n",
        "Convolution:\n",
        "Convolution is a fundamental operation in CNNs where a filter/kernel slides over an input image to extract features.\n",
        "The filter applies element-wise multiplication and sums the results to produce a feature map.\n",
        "\n",
        "Filter/Kernel:\n",
        "A small matrix used in the convolution operation to extract specific features from an input image.\n",
        "Each filter is responsible for detecting a particular pattern, such as edges or textures.\n",
        "\n",
        "Feature Map:\n",
        "The output of a convolutional layer after applying multiple filters to an input image.\n",
        "It represents the presence of specific features at different spatial locations.\n",
        "\n",
        "Stride:\n",
        "Stride is the step size at which a filter moves horizontally or vertically during the convolution operation.\n",
        "A larger stride reduces the spatial dimensions of the feature map, affecting the output size.\n",
        "\n",
        "Padding:\n",
        "Padding involves adding extra pixels around the input image to preserve spatial dimensions after convolution.\n",
        "It helps avoid information loss at the borders of the image.\n",
        "\n",
        "Pooling:\n",
        "Pooling is a downsampling technique used to reduce the spatial dimensions of the feature map.\n",
        "Common pooling types include Max Pooling (selecting the maximum value in a local region) and Average Pooling (calculating the average value).\n",
        "\n",
        "Activation Function:\n",
        "An activation function introduces non-linearity to the output of a neuron, enabling the network to learn complex relationships.\n",
        "Common activation functions in CNNs include ReLU (Rectified Linear Unit) and sigmoid.\n",
        "\n",
        "Fully Connected Layer:\n",
        "After convolutional and pooling layers, fully connected layers process flattened feature maps to make predictions.\n",
        "These layers connect every neuron to every neuron in the previous and subsequent layers.\n",
        "\n",
        "Backpropagation:\n",
        "Backpropagation is the training process in which the network adjusts its parameters (weights and biases) based on the calculated error during training.\n",
        "It uses optimization algorithms like gradient descent to minimize the loss function.\n",
        "\n",
        "Loss Function:\n",
        "The loss function measures the difference between predicted and actual outputs during training.\n",
        "It helps the network to adjust its parameters to minimize the error.\n",
        "\n",
        "Activation Map:\n",
        "The output of a layer after applying the activation function, such as ReLU, is known as the activation map.\n",
        "It highlights the regions of the input that are most important for making predictions.\n",
        "\n",
        "Transfer Learning:\n",
        "Transfer learning is a technique where a pre-trained CNN model is used as a starting point for a new, related task.\n",
        "By leveraging knowledge from a larger dataset, it can speed up training and improve performance on the new task.\n",
        "\n",
        "Data Augmentation:\n",
        "Data augmentation is a method to increase the diversity of the training data by applying transformations (e.g., rotation, scaling) to the images.\n",
        "It helps prevent overfitting and improves the model's ability to generalize.\n",
        "\n",
        "Epoch:\n",
        "An epoch is one complete cycle of training, where the entire training dataset is fed to the network once.\n",
        "\n",
        "Training usually involves running multiple epochs to allow the model to learn from the data iteratively.\n",
        "Convolutional Neural Networks have revolutionized computer vision tasks and are widely used in various applications, from object detection to image segmentation."
      ],
      "metadata": {
        "id": "giV1K0u4yg9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A perceptron is the simplest form of an artificial neural network (ANN) introduced by Frank Rosenblatt\n",
        "in the late 1950s. It is the building block of more complex neural network architectures.\n",
        "Let's explain the perceptron ANN:\n",
        "\n",
        "Neuron/Perceptron:\n",
        "\n",
        "The perceptron is a simplified model of a biological neuron.\n",
        "It takes one or more inputs, multiplies them by corresponding weights, and produces an output.\n",
        "Inputs and Weights:\n",
        "\n",
        "The perceptron receives multiple input values (x1, x2, ..., xn) and assigns a weight (w1, w2, ..., wn) to each input.\n",
        "The weights represent the importance of each input in determining the output.\n",
        "\n",
        "Summation Function:\n",
        "The perceptron computes the weighted sum of the inputs and weights.\n",
        "This operation is also known as the \"weighted sum\" or \"activation.\"\n",
        "\n",
        "Activation Function:\n",
        "The output of the weighted sum is passed through an activation function, which introduces non-linearity to the perceptron.\n",
        "The activation function determines whether the perceptron should \"fire\" (produce an output) or remain inactive (output 0).\n",
        "\n",
        "Bias (Optional):\n",
        "In some versions of the perceptron, a bias term (often denoted as \"b\") is introduced.\n",
        "The bias is like an intercept term that helps control the decision boundary independently of the input data.\n",
        "\n",
        "Decision Boundary:\n",
        "The weights and bias in the perceptron define a decision boundary in the input space.\n",
        "The perceptron will classify inputs based on which side of the decision boundary they fall.\n",
        "\n",
        "Training:\n",
        "During the training process, the perceptron adjusts its weights and bias based on the\n",
        "training data to minimize the classification error.\n",
        "The perceptron learning algorithm iteratively updates the weights and bias to move the\n",
        "decision boundary towards the correct classification.\n",
        "\n",
        "Linear Separability:\n",
        "A crucial limitation of the perceptron is that it can only learn and converge on\n",
        "a solution if the input data is linearly separable, meaning a straight line\n",
        "(or hyperplane in higher dimensions) can separate the two classes.\n",
        "\n",
        "Single-Layer Perceptron:\n",
        "The perceptron with one layer of neurons is called a \"single-layer perceptron.\"\n",
        "It can only solve linearly separable problems and is not capable\n",
        "of learning complex patterns or handling non-linearly separable data.\n",
        "\n",
        "Multilayer Perceptrons (MLPs):\n",
        "To overcome the limitations of single-layer perceptrons, researchers developed multilayer perceptrons (MLPs)\n",
        "with multiple layers of interconnected neurons.\n",
        "MLPs can learn complex patterns and solve non-linearly separable problems by\n",
        "introducing hidden layers and non-linear activation functions.\n",
        "\n",
        "Activation Functions in Modern Perceptrons:\n",
        "In modern neural networks, common activation functions used in perceptrons include\n",
        "ReLU (Rectified Linear Unit), sigmoid, tanh, and others\n",
        "\n",
        "Historical Significance:\n",
        "The perceptron is historically significant as it was one of the earliest models of\n",
        "artificial neural networks and played a pivotal role in the development of the\n",
        "field of artificial intelligence and machine learning.\n",
        "\n",
        "In summary, the perceptron is a fundamental unit of an artificial neural network.\n",
        "While its original form has limitations, it served as the foundation for the\n",
        "development of more advanced neural network architectures, like multilayer\n",
        "perceptrons, which are widely used for various complex tasks in modern machine learning."
      ],
      "metadata": {
        "id": "Kf5mxnJI2Krk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ucse5pir2FLb"
      }
    }
  ]
}